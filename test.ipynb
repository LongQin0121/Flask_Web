{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "    Feature1  Feature2  Target\n",
      "0          1         5       0\n",
      "1          2         4       0\n",
      "2          3         3       2\n",
      "3          4         2       0\n",
      "4          5         1       0\n",
      "5          6         5       1\n",
      "6          7         4       1\n",
      "7          8         3       2\n",
      "8          9         2       1\n",
      "9         10         1       1\n",
      "10        11         5       0\n",
      "11        12         4       0\n",
      "12        13         3       2\n",
      "13        14         2       1\n",
      "14        15         1       1\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 1]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a small dataset\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    'Feature2': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1, 5, 4, 3, 2, 1],\n",
    "    'Target': [0, 0, 2, 0, 0, 1, 1, 2, 1, 1, 0, 0, 2, 1, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X = df[['Feature1', 'Feature2']]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training a Logistic Regression model with multi_class='auto'\n",
    "model = LogisticRegression(multi_class='auto', max_iter=1000)  # Added max_iter for convergence warning\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculating confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Displaying the dataset and confusion matrix\n",
    "print(\"Dataset:\")\n",
    "print(df)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass-multioutput and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m predictions \u001b[38;5;241m=\u001b[39m multi_output_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluate the accuracy\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, predictions)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Display example predictions and actual values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qinlo\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\qinlo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\qinlo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example dataset with 10 rows and 5 columns\n",
    "X = np.array([\n",
    "    [1.2, 2.3, 0.8, 4.1, 5.2],\n",
    "    [0.5, 1.8, 2.0, 3.2, 2.9],\n",
    "    [2.0, 1.0, 1.5, 5.0, 4.7],\n",
    "    [1.8, 2.5, 0.7, 3.8, 6.2],\n",
    "    [0.9, 1.5, 2.2, 4.5, 3.8],\n",
    "    [1.5, 1.2, 1.9, 3.9, 4.0],\n",
    "    [2.2, 2.8, 1.0, 4.8, 5.5],\n",
    "    [1.0, 1.5, 2.5, 4.0, 3.2],\n",
    "    [1.7, 2.0, 0.5, 3.5, 6.0],\n",
    "    [2.5, 1.7, 1.2, 4.2, 4.9]\n",
    "])\n",
    "\n",
    "# Binary target variable 1\n",
    "y1 = np.array([0, 1, 1, 0, 1, 0, 1, 0, 0, 1])\n",
    "\n",
    "# Multiclass target variable 2\n",
    "y2 = np.array([2, 0, 1, 1, 0, 2, 1, 0, 2, 1])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(\n",
    "    X, y1, y2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create multi-output classifiers\n",
    "classifier1 = RandomForestClassifier(random_state=42)\n",
    "classifier2 = RandomForestClassifier(random_state=42)\n",
    "\n",
    "multi_output_classifier = MultiOutputClassifier(\n",
    "    estimators=[('classifier1', classifier1), ('classifier2', classifier2)]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "multi_output_classifier.fit(X_train, np.column_stack((y1_train, y2_train)))\n",
    "\n",
    "# Make predictions\n",
    "predictions = multi_output_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(np.column_stack((y1_test, y2_test)), predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display example predictions and actual values\n",
    "print(\"\\nExample Predictions vs Actual Values:\")\n",
    "for i in range(len(X_test)):\n",
    "    print(f\"Sample {i + 1}: Predicted - {predictions[i]}, Actual - {np.column_stack((y1_test, y2_test))[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(10, 1)\n",
    "y = 4 + 3 * X + np.random.randn(10, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
